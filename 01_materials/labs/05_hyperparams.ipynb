{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we doing?\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Construct a cross-validation pipeline.\n",
    "+ Use cross-validation to evaluate different hyperparameter performance.\n",
    "+ Perform grid search for systemic evaluation.\n",
    "+ Store and manage results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "The diagram below, taken from Scikit Learn's documentation, shows the procedure that we will follow:\n",
    "\n",
    "![](./images/05_grid_search_workflow.png)\n",
    "\n",
    "\n",
    "+ System requriements:\n",
    "    \n",
    "    - Automation: the system should operate automatically with the least amount of supervision. \n",
    "    - Replicability: changes to code and (arguably) data should be logged and controled. Randomness should also be controlled (random seeds, etc.)\n",
    "    - Persistence: persist results for later analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Hyperparameter?\n",
    "\n",
    "+ Generally speaking, hyperparameters are parameters that control the learning process: regularization weights, learning rate, entropy/gini metrics, etc. \n",
    "+ Hyperparameters will drive the behaviour and performance of a model. Model selection is intimately related with hyperparameter tuning. \n",
    "+ Selection critieria are based on performance evaluation and, to get better performance estimates, we use cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the Hyperparameter Grid\n",
    "\n",
    "+ To address the automation requirement, we could use `GridSearchCV()`, which is a self-contained function for performing a Grid Search over a hyperparameter space.\n",
    "+ To \"Search the Hyperparameter Grid\" exhaustively means that we will consider all possible combination of hyperparameter values in the search space and evaluate the model using those hyperparams. For example, if we have two parameters that we are exploring, kernel (takes values \"rbf\" and \"poly\") and C (takes values 1.0 and 0.5), then this grid would be the combinations:\n",
    "\n",
    "    + (rbf, 1.0)\n",
    "    + (rbf, 0.5)\n",
    "    + (poly, 1.0)\n",
    "    + (poly, 0.5)\n",
    "\n",
    "+ Under each combination, we perform CV and evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "We start with [Give me some credit](https://www.kaggle.com/c/GiveMeSomeCredit) data that we used in the previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "# System & OS utilities\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project source directory\n",
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "\n",
    "# Scikit-learn: Preprocessing, Pipelines, and Model Selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    ParameterGrid\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    roc_auc_score, \n",
    "    roc_curve, \n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Parallel processing & Progress Bar\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Debugging & Logging\n",
    "import logging\n",
    "\n",
    "# Load environment variables\n",
    "log_dir = os.getenv('LOG_DIR')\n",
    "db_url = os.getenv('DB_URL')\n",
    "tickers_file = os.getenv('TICKERS')\n",
    "\n",
    "print(f\"Log Directory: {log_dir}\")\n",
    "print(f\"Tickers File: {tickers_file}\")\n",
    "print(f\"Database URL: {db_url}\")\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"DEBUG: DB_URL =\", os.getenv(\"DB_URL\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.drop(columns = [\"Unnamed: 0\"]).rename(\n",
    "    columns = {\n",
    "        'SeriousDlqin2yrs': 'delinquency',\n",
    "        'RevolvingUtilizationOfUnsecuredLines': 'revolving_unsecured_line_utilization', \n",
    "        'age': 'age',\n",
    "        'NumberOfTime30-59DaysPastDueNotWorse': 'num_30_59_days_late', \n",
    "        'DebtRatio': 'debt_ratio', \n",
    "        'MonthlyIncome': 'monthly_income',\n",
    "        'NumberOfOpenCreditLinesAndLoans': 'num_open_credit_loans', \n",
    "        'NumberOfTimes90DaysLate':  'num_90_days_late',\n",
    "        'NumberRealEstateLoansOrLines': 'num_real_estate_loans', \n",
    "        'NumberOfTime60-89DaysPastDueNotWorse': 'num_60_89_days_late',\n",
    "        'NumberOfDependents': 'num_dependents'\n",
    "    }\n",
    ").assign(\n",
    "    high_debt_ratio = lambda x: (x['debt_ratio'] > 1)*1,\n",
    "    missing_monthly_income = lambda x: x['monthly_income'].isna()*1,\n",
    "    missing_num_dependents = lambda x: x['num_dependents'].isna()*1, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a simple pipeline composed of:\n",
    "\n",
    "+ Preprocessing steps.\n",
    "+ Logistic Regression classifier.\n",
    "\n",
    "We will explore the hyperparameter sapce by evaluating different regularization strategies and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameWrapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Ensures transformed NumPy arrays are converted back to Pandas DataFrames while keeping expected column names\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_names = None  \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Store feature names if input is a DataFrame\"\"\"\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.feature_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Convert array back to DataFrame with stored feature names\"\"\"\n",
    "        if self.feature_names is not None:\n",
    "            return pd.DataFrame(X, columns=self.feature_names)\n",
    "        else:\n",
    "            return pd.DataFrame(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature matrix X and target variable Y\n",
    "X = df.drop(columns='delinquency')\n",
    "Y = df['delinquency']\n",
    "\n",
    "# Ensure X_train and X_test are DataFrames before training\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['revolving_unsecured_line_utilization', 'age',\n",
    "       'num_30_59_days_late', 'debt_ratio', 'monthly_income',\n",
    "       'num_open_credit_loans', 'num_90_days_late', 'num_real_estate_loans',\n",
    "       'num_60_89_days_late', 'num_dependents', \n",
    "       # Although expressed as numbers, these columns are boolean:\n",
    "       # 'high_debt_ratio',\n",
    "       # 'missing_monthly_income', \n",
    "       # 'missing_num_dependents' \n",
    "       ]\n",
    "\n",
    "pipe_num_simple = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('standardizer', StandardScaler())\n",
    "])\n",
    "\n",
    "ctransform_simple= ColumnTransformer(\n",
    "    [('numeric_simple', pipe_num_simple, num_cols),],\n",
    "    remainder='passthrough',force_int_remainder_cols=False)\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),  # Standardizes numerical features\n",
    "    (\"preprocess\", ctransform_simple),  # Applies transformations\n",
    "    (\"clf\", LogisticRegression())  # Model\n",
    "])\n",
    "\n",
    "\n",
    "pipe_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the parameters of the pipeline with `.get_params()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.get_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Splitting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(columns = 'delinquency')\n",
    "# Y = df['delinquency']\n",
    "\n",
    "# scoring = ['neg_log_loss', 'roc_auc', 'f1', 'accuracy', 'precision', 'recall']\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the Grid Search we need to define a parameter grid:\n",
    "\n",
    "- A parameter grid defines all of the combinations of parameters that we need to explore.\n",
    "- The function `GridSearchCV()` performs an exhaustive search of parameter combinations.\n",
    "- The parameter grid is defined as a dictionary of lists:\n",
    "\n",
    "    * Each entry's key is the name of the parameter.\n",
    "    * Each entry's value is the list of values that we would like to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_penalty_map = {\n",
    "    \"lbfgs\": {\"penalty\": [\"l2\"]},  # lbfgs only supports L2\n",
    "    \"liblinear\": {\"penalty\": [\"l1\", \"l2\"]},  # Supports L1 and L2\n",
    "    \"saga\": {\"penalty\": [\"l1\", \"l2\", \"elasticnet\"], \"l1_ratio\": [0.1, 0.5, 0.9]},  # Needs l1_ratio for elasticnet\n",
    "    \"newton-cg\": {\"penalty\": [\"l2\"]},  # Only supports L2\n",
    "    \"sag\": {\"penalty\": [\"l2\"]},  # Only supports L2\n",
    "}\n",
    "\n",
    "param_grid = []\n",
    "for solver, params in solver_penalty_map.items():\n",
    "    grid = {\n",
    "        \"clf__solver\": [solver], \n",
    "        \"clf__penalty\": params[\"penalty\"],\n",
    "        \"clf__C\": [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    }\n",
    "    if \"l1_ratio\" in params:\n",
    "        grid[\"clf__l1_ratio\"] = params[\"l1_ratio\"]\n",
    "    param_grid.append(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"clf__C\": [0.001, 0.01, 0.1, 0.5, 1.0],  # Regularization strength\n",
    "#     \"clf__penalty\": [\"l1\", \"l2\"]#,  # Penalty type\n",
    "#     #\"clf__solver\": [\"liblinear\", \"saga\"],  # Only solvers that support both l1 and l2\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key inputs to [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) are:\n",
    "\n",
    "+ `estimator`: the pipeline or classifier that we are tuning.\n",
    "+ `param_grid`: the parameter grid defined as a dictionary of lists described above.\n",
    "+ `n_jobs`: settings for parallel computation.\n",
    "+ `refit`: options for refitting the model using the best-performing configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['neg_log_loss', 'roc_auc', 'f1', 'accuracy', 'precision', 'recall']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv = GridSearchCV(\n",
    "    estimator=pipe_lr, \n",
    "    param_grid=param_grid, \n",
    "    scoring=scoring, \n",
    "    cv=5,\n",
    "    refit=\"neg_log_loss\",\n",
    "    n_jobs=-1,\n",
    "    verbose=3  # Enables built-in progress updates\n",
    ")\n",
    "\n",
    "grid_cv.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the cross-validation results using the property `.cv_results_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = grid_cv.cv_results_\n",
    "res = pd.DataFrame(res)\n",
    "res.columns\n",
    "\n",
    "res[['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
    "       'param_clf__C', 'param_clf__penalty', 'param_clf__solver', 'params',\n",
    "       'mean_test_neg_log_loss',\n",
    "       'std_test_neg_log_loss', 'rank_test_neg_log_loss']].sort_values('rank_test_neg_log_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(grid_cv.cv_results_)\n",
    "print(res.columns)  # Verify available columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(grid_cv.cv_results_)\n",
    "\n",
    "# Drop invalid results (failed fits)\n",
    "res_filtered = res.dropna(subset=[\"mean_test_neg_log_loss\"])\n",
    "\n",
    "# Display best results\n",
    "print(\"\\n🔹 Best Results:\")\n",
    "print(res_filtered.nsmallest(5, \"rank_test_neg_log_loss\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the best-performing configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing classifier (pipeline) trained on the complete training set is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking GridSearchCV Experiments\n",
    "\n",
    "+ We can expand our infrastructure for hyperparameter tuning across various models.\n",
    "+ The plan:\n",
    "\n",
    "    - Create a model ingredient to obtain the classifier object.\n",
    "    - Create experiment param grids in json files to organize our parameter grids.\n",
    "    - Schedule the experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Design\n",
    "\n",
    "<div>\n",
    "<img src=./images/05_experiment_setup.png width=\"75%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the code in `./05_src/credit_experiment.py` and `./05_src/credit_model_ingredient.py`:\n",
    "\n",
    "+ `credit_model_ingredient.py` implements a function that returns a model given a string. This way, we can parametrize models in the experiment.\n",
    "+ `credit_experiment.py` is modularized version of our previous file, `credit_experiment_nb.py` which only worked with Naive Bayes classifier.\n",
    "+ The experiment is now further *modularized*: there are ingredients for most components and it can be broken down even more depending on the evolution of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiments from the Command Line\n",
    "\n",
    "Access the experiment through the [Command Line Interface](https://sacred.readthedocs.io/en/stable/command_line.html).\n",
    "\n",
    "```\n",
    "cd src  # if required\n",
    "python credit_experiment.py\n",
    "```\n",
    "\n",
    "We can also change the parameters of the experiment. For instance, using the same code, we can run an experiment with a logistic regression classifier using a basic (not power) preprocessing pipeline:\n",
    "\n",
    "```\n",
    "python .\\credit_experiment.py with 'preprocessing=\"basic\"' 'model=\"LogisticRegression\"'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few Notes About Sacred\n",
    "\n",
    "+ Sacred is a powerful tool, but it is only the beginning.  \n",
    "+ Sacred is useful in keeping track of experiments within a limited scope: it is not a project management tool.\n",
    "+ It works well in SQL environments, but handling hyperparameters can be painful.\n",
    "+ The natural backend is MongoDB, however not all workplaces have running instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Schema\n",
    "\n",
    "The database schema implemented by sacred is shown below. The schema is a useful representation of the code and setup of an experiment. The package offers a [metrics API](https://sacred.readthedocs.io/en/stable/collected_information.html#metrics-api), but we have decided to extend the framework with a few ad-hoc tables with performance metrics. \n",
    "\n",
    "The database backend is a database like any other: you can query it with Python, R, or PowerBI.\n",
    "\n",
    "+ Server is located in localhost port 5432.\n",
    "+ User and password are in the .env file in `./05_src/db/`.\n",
    "\n",
    "<div>\n",
    "<img src=./images/05_sacred_sql_schema.png width=\"40%\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
