{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Directory: ../../07_logs/\n",
      "Tickers File: ../../05_src/data/tickers/sp500_wiki.csv\n",
      "Database URL: postgresql://postgres:HumanAfterAll@localhost:5432/model_db\n",
      "Current working directory: c:\\Users\\AC\\Documents\\GitHub\\Programs\\UofT-DSI\\production\\01_materials\\labs\n",
      "DEBUG: DB_URL = postgresql://postgres:HumanAfterAll@localhost:5432/model_db\n",
      "Pipeline:\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('preprocess',\n",
      "                 ColumnTransformer(force_int_remainder_cols=False,\n",
      "                                   remainder='passthrough',\n",
      "                                   transformers=[('numeric_simple',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('standardizer',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['revolving_unsecured_line_utilization',\n",
      "                                                   'age', 'num_30_59_days_late',\n",
      "                                                   'debt_ratio',\n",
      "                                                   'monthly_income',\n",
      "                                                   'num_open_credit_loans',\n",
      "                                                   'num_90_days_late',\n",
      "                                                   'num_real_estate_loans',\n",
      "                                                   'num_60_89_days_late',\n",
      "                                                   'num_dependents'])])),\n",
      "                ('clf', LogisticRegression(max_iter=500))])\n",
      "\n",
      "Starting Grid Search...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "Available Columns in Results:\n",
      "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
      "       'param_clf__C', 'param_clf__max_iter', 'param_clf__penalty',\n",
      "       'param_clf__solver', 'param_clf__l1_ratio', 'params',\n",
      "       'split0_test_neg_log_loss', 'split1_test_neg_log_loss',\n",
      "       'split2_test_neg_log_loss', 'split3_test_neg_log_loss',\n",
      "       'split4_test_neg_log_loss', 'mean_test_neg_log_loss',\n",
      "       'std_test_neg_log_loss', 'rank_test_neg_log_loss',\n",
      "       'split0_test_roc_auc', 'split1_test_roc_auc', 'split2_test_roc_auc',\n",
      "       'split3_test_roc_auc', 'split4_test_roc_auc', 'mean_test_roc_auc',\n",
      "       'std_test_roc_auc', 'rank_test_roc_auc', 'split0_test_f1',\n",
      "       'split1_test_f1', 'split2_test_f1', 'split3_test_f1', 'split4_test_f1',\n",
      "       'mean_test_f1', 'std_test_f1', 'rank_test_f1', 'split0_test_accuracy',\n",
      "       'split1_test_accuracy', 'split2_test_accuracy', 'split3_test_accuracy',\n",
      "       'split4_test_accuracy', 'mean_test_accuracy', 'std_test_accuracy',\n",
      "       'rank_test_accuracy', 'split0_test_precision', 'split1_test_precision',\n",
      "       'split2_test_precision', 'split3_test_precision',\n",
      "       'split4_test_precision', 'mean_test_precision', 'std_test_precision',\n",
      "       'rank_test_precision', 'split0_test_recall', 'split1_test_recall',\n",
      "       'split2_test_recall', 'split3_test_recall', 'split4_test_recall',\n",
      "       'mean_test_recall', 'std_test_recall', 'rank_test_recall'],\n",
      "      dtype='object')\n",
      "\n",
      "Top 5 Best Results (by rank_test_neg_log_loss):\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "12       0.414203      0.010332         0.064167        0.008829   \n",
      "23       5.551076      0.353570         0.063485        0.005144   \n",
      "13       0.497173      0.058228         0.061700        0.007376   \n",
      "21       5.646065      0.828544         0.112670        0.024928   \n",
      "24       0.966130      0.241435         0.108859        0.021956   \n",
      "\n",
      "    param_clf__C  param_clf__max_iter param_clf__penalty param_clf__solver  \\\n",
      "12           1.0                  500                 l2             lbfgs   \n",
      "23           1.0                  500                 l1         liblinear   \n",
      "13           1.0                  500                 l2         newton-cg   \n",
      "21           0.5                  500                 l1         liblinear   \n",
      "24           1.0                  500                 l2         liblinear   \n",
      "\n",
      "    param_clf__l1_ratio                                             params  \\\n",
      "12                  NaN  {'clf__C': 1.0, 'clf__max_iter': 500, 'clf__pe...   \n",
      "23                  NaN  {'clf__C': 1.0, 'clf__max_iter': 500, 'clf__pe...   \n",
      "13                  NaN  {'clf__C': 1.0, 'clf__max_iter': 500, 'clf__pe...   \n",
      "21                  NaN  {'clf__C': 0.5, 'clf__max_iter': 500, 'clf__pe...   \n",
      "24                  NaN  {'clf__C': 1.0, 'clf__max_iter': 500, 'clf__pe...   \n",
      "\n",
      "    ...  std_test_precision  rank_test_precision  split0_test_recall  \\\n",
      "12  ...            0.016801                   20            0.040273   \n",
      "23  ...            0.021809                   27            0.041512   \n",
      "13  ...            0.019040                   11            0.040273   \n",
      "21  ...            0.019555                   24            0.040892   \n",
      "24  ...            0.019464                   14            0.040273   \n",
      "\n",
      "    split1_test_recall  split2_test_recall  split3_test_recall  \\\n",
      "12            0.050186            0.047088            0.042751   \n",
      "23            0.050805            0.047708            0.043371   \n",
      "13            0.050186            0.047708            0.043371   \n",
      "21            0.050186            0.047708            0.043371   \n",
      "24            0.050186            0.047708            0.043371   \n",
      "\n",
      "    split4_test_recall  mean_test_recall  std_test_recall  rank_test_recall  \n",
      "12            0.048947          0.045849         0.003759                 5  \n",
      "23            0.048947          0.046468         0.003483                 1  \n",
      "13            0.048947          0.046097         0.003709                 3  \n",
      "21            0.048947          0.046221         0.003518                 2  \n",
      "24            0.048947          0.046097         0.003709                 3  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "\n",
      "Best Parameters:\n",
      "{'clf__C': 1.0, 'clf__max_iter': 500, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97     28044\n",
      "           1       0.50      0.04      0.07      1956\n",
      "\n",
      "    accuracy                           0.93     30000\n",
      "   macro avg       0.72      0.52      0.52     30000\n",
      "weighted avg       0.91      0.93      0.91     30000\n",
      "\n",
      "ROC AUC Score: 0.6927427291440066\n",
      "\n",
      "Model and results saved.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 1. Environment Setup and Imports\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "# System & OS utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the project source directory if needed\n",
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "\n",
    "# Scikit-learn configuration: Force transformers to return DataFrames\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "# Scikit-learn: Preprocessing, Pipelines, Model Selection, and Metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    ParameterGrid\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# For parallel processing and progress monitoring\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# For model persistence\n",
    "import joblib\n",
    "\n",
    "# Optionally, set warnings to show by default\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Logging Environment Information\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "log_dir = os.getenv('LOG_DIR')\n",
    "db_url = os.getenv('DB_URL')\n",
    "tickers_file = os.getenv('TICKERS')\n",
    "\n",
    "print(f\"Log Directory: {log_dir}\")\n",
    "print(f\"Tickers File: {tickers_file}\")\n",
    "print(f\"Database URL: {db_url}\")\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"DEBUG: DB_URL =\", os.getenv(\"DB_URL\"))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Data Loading and Preprocessing\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Load the GiveMeSomeCredit dataset\n",
    "ft_path = os.getenv(\"CREDIT_DATA\")\n",
    "df_raw = pd.read_csv(ft_path)\n",
    "\n",
    "# Rename columns and create additional features\n",
    "df = df_raw.drop(columns=[\"Unnamed: 0\"]).rename(\n",
    "    columns={\n",
    "        'SeriousDlqin2yrs': 'delinquency',\n",
    "        'RevolvingUtilizationOfUnsecuredLines': 'revolving_unsecured_line_utilization',\n",
    "        'age': 'age',\n",
    "        'NumberOfTime30-59DaysPastDueNotWorse': 'num_30_59_days_late',\n",
    "        'DebtRatio': 'debt_ratio',\n",
    "        'MonthlyIncome': 'monthly_income',\n",
    "        'NumberOfOpenCreditLinesAndLoans': 'num_open_credit_loans',\n",
    "        'NumberOfTimes90DaysLate': 'num_90_days_late',\n",
    "        'NumberRealEstateLoansOrLines': 'num_real_estate_loans',\n",
    "        'NumberOfTime60-89DaysPastDueNotWorse': 'num_60_89_days_late',\n",
    "        'NumberOfDependents': 'num_dependents'\n",
    "    }\n",
    ").assign(\n",
    "    high_debt_ratio=lambda x: (x['debt_ratio'] > 1) * 1,\n",
    "    missing_monthly_income=lambda x: x['monthly_income'].isna() * 1,\n",
    "    missing_num_dependents=lambda x: x['num_dependents'].isna() * 1\n",
    ")\n",
    "\n",
    "# Define feature matrix X and target variable Y\n",
    "X = df.drop(columns='delinquency')\n",
    "Y = df['delinquency']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "# Ensure they remain DataFrames\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "# Define numerical columns for transformation\n",
    "num_cols = [\n",
    "    'revolving_unsecured_line_utilization', 'age',\n",
    "    'num_30_59_days_late', 'debt_ratio', 'monthly_income',\n",
    "    'num_open_credit_loans', 'num_90_days_late', 'num_real_estate_loans',\n",
    "    'num_60_89_days_late', 'num_dependents'\n",
    "]\n",
    "\n",
    "# Build a simple numeric preprocessing pipeline\n",
    "pipe_num_simple = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('standardizer', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create a ColumnTransformer that applies the numeric pipeline to num_cols\n",
    "ctransform_simple = ColumnTransformer(\n",
    "    transformers=[('numeric_simple', pipe_num_simple, num_cols)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. Pipeline Construction\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Create a pipeline for Logistic Regression\n",
    "pipe_lr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"preprocess\", ctransform_simple),\n",
    "    (\"clf\", LogisticRegression(max_iter=500, tol=1e-4))\n",
    "])\n",
    "\n",
    "print(\"Pipeline:\")\n",
    "print(pipe_lr)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Construct a Refined Hyperparameter Grid\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Define separate grids to avoid l1_ratio warnings and improve convergence.\n",
    "# Grid 1: For lbfgs, newton-cg, sag (which support only l2 penalty)\n",
    "grid1 = {\n",
    "    \"clf__solver\": [\"lbfgs\", \"newton-cg\", \"sag\"],\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__C\": [0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "    \"clf__max_iter\": [500]\n",
    "}\n",
    "\n",
    "# Grid 2: For liblinear (supports l1 and l2 without l1_ratio)\n",
    "grid2 = {\n",
    "    \"clf__solver\": [\"liblinear\"],\n",
    "    \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "    \"clf__C\": [0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "    \"clf__max_iter\": [500]\n",
    "}\n",
    "\n",
    "# Grid 3: For saga with l1 and l2 penalties (without l1_ratio)\n",
    "grid3 = {\n",
    "    \"clf__solver\": [\"saga\"],\n",
    "    \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "    \"clf__C\": [0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "    \"clf__max_iter\": [500]\n",
    "}\n",
    "\n",
    "# Grid 4: For saga with elasticnet penalty (requires l1_ratio)\n",
    "grid4 = {\n",
    "    \"clf__solver\": [\"saga\"],\n",
    "    \"clf__penalty\": [\"elasticnet\"],\n",
    "    \"clf__C\": [0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "    \"clf__l1_ratio\": [0.1, 0.5, 0.9],\n",
    "    \"clf__max_iter\": [500]\n",
    "}\n",
    "\n",
    "# Combine grids into one list\n",
    "param_grid = [grid1, grid2, grid3, grid4]\n",
    "\n",
    "# Define scoring metrics (multiple metrics are computed)\n",
    "scoring = ['neg_log_loss', 'roc_auc', 'f1', 'accuracy', 'precision', 'recall']\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Grid Search with Parallel Processing and Verbose Output\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "grid_cv = GridSearchCV(\n",
    "    estimator=pipe_lr,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=5,\n",
    "    refit=\"neg_log_loss\",\n",
    "    n_jobs=-1,\n",
    "    verbose=3  # Built-in progress updates\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Grid Search...\")\n",
    "grid_cv.fit(X_train, Y_train)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Results and Evaluation\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Convert CV results to DataFrame\n",
    "results_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "print(\"\\nAvailable Columns in Results:\")\n",
    "print(results_df.columns)\n",
    "\n",
    "# Optionally, filter out failed fits (if any nan scores)\n",
    "results_df_filtered = results_df.dropna(subset=[\"mean_test_neg_log_loss\"])\n",
    "print(\"\\nTop 5 Best Results (by rank_test_neg_log_loss):\")\n",
    "print(results_df_filtered.nsmallest(5, \"rank_test_neg_log_loss\"))\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(grid_cv.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "Y_pred = grid_cv.best_estimator_.predict(X_test)\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(Y_test, grid_cv.best_estimator_.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Save the Best Model and Results\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "joblib.dump(grid_cv.best_estimator_, os.path.join(log_dir, \"best_model.pkl\"))\n",
    "results_df.to_csv(os.path.join(log_dir, \"grid_search_results.csv\"), index=False)\n",
    "\n",
    "print(\"\\nModel and results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
